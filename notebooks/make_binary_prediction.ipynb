{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and dependencies\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "import os\n",
    "import numpy as np \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your own directory location\n",
    "\n",
    "test_dir = 'F:/Nilgai_photo_database/Nilgai Classifier/Nilgai/sample_images/'\n",
    "img_dir = 'F:/Nilgai_photo_database/Nilgai Classifier/Nilgai/sample_images/images/'\n",
    "pred_dir = 'F:/Nilgai_photo_database/Nilgai Classifier/Nilgai/predictions/'\n",
    "binary_model_path = 'F:/Nilgai_photo_database/Nilgai Classifier/Nilgai/models/Final_binary_nil_05262020.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "\n",
    "def img_crop(img):\n",
    "    # for cropping\n",
    "    offset_width = 0\n",
    "    offset_height = 9\n",
    "    target_height = 273\n",
    "    target_width = 299\n",
    "\n",
    "    new_img = tf.image.crop_to_bounding_box(img, offset_height, offset_width, target_height, target_width)\n",
    "    new_img = tf.image.resize(new_img, (299,299))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 100 images belonging to 1 classes.\n"
    }
   ],
   "source": [
    "# tensorflow data loaders\n",
    "img_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, preprocessing_function=img_crop)\n",
    "\n",
    "test_generator = img_generator.flow_from_directory(directory=str(test_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=False,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     class_mode= None)\n",
    "# load model\n",
    "binary_model = tf.keras.models.load_model(binary_model_path, custom_objects={'KerasLayer':hub.KerasLayer}, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100/100 [==============================] - 2s 17ms/step\n"
    }
   ],
   "source": [
    "# make predictions\n",
    "pred_steps=np.ceil(test_generator.samples/test_generator.batch_size)\n",
    "\n",
    "pred = binary_model.predict(test_generator, verbose=1, workers=4, steps=pred_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Not_Nilgai'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# define class names and create and index for future mapping\n",
    "predicted_class_indices = tf.round(pred).numpy().flatten()\n",
    "class_names = {'Nilgai': 0, 'Not_Nilgai': 1}\n",
    "labels = dict((v,k) for k,v in class_names.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "# predicted_id = np.round(pred).astype(int).flatten()\n",
    "# predicted_label_batch = class_names[predicted_id]\n",
    "# predicted_label_batch\n",
    "predictions[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The \"Nilgai\" folder already exists. Check your prediction path.\nThe \"Not_Nilgai\" folder already exists. Check your prediction path.\n"
    }
   ],
   "source": [
    "for i in class_names:\n",
    "    img_path = os.path.join(pred_dir, str(i))\n",
    "    try:\n",
    "        os.makedirs(img_path, exist_ok=False)\n",
    "    except:\n",
    "        print(\"The \" + '\"' + str(i) + '\"' + \" folder already exists. Check your prediction path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{0: 'Nilgai', 1: 'Not_Nilgai'}\n0\n0.jpg\n0\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n1\n1.jpg\n1\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n10\n10.jpg\n2\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n11\n11.jpg\n3\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n12\n12.jpg\n4\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n13\n13.jpg\n5\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n14\n14.jpg\n6\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n15\n15.jpg\n7\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n16\n16.jpg\n8\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n17\n17.jpg\n9\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n18\n18.jpg\n10\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n19\n19.jpg\n11\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n2\n2.jpg\n12\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n20\n20.jpg\n13\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n21\n21.jpg\n14\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n22\n22.jpg\n15\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n23\n23.jpg\n16\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n24\n24.jpg\n17\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n25\n25.jpg\n18\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n26\n26.jpg\n19\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n27\n27.jpg\n20\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n28\n28.jpg\n21\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n29\n29.jpg\n22\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n3\n3.jpg\n23\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n30\n30.jpg\n24\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n31\n31.jpg\n25\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n32\n32.jpg\n26\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n33\n33.jpg\n27\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n34\n34.jpg\n28\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n35\n35.jpg\n29\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n36\n36.jpg\n30\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n37\n37.jpg\n31\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n38\n38.jpg\n32\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n39\n39.jpg\n33\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n4\n4.jpg\n34\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n40\n40.jpg\n35\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n41\n41.jpg\n36\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n42\n42.jpg\n37\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n43\n43.jpg\n38\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n44\n44.jpg\n39\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n45\n45.jpg\n40\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n46\n46.jpg\n41\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n47\n47.jpg\n42\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n48\n48.jpg\n43\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n49\n49.jpg\n44\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n5\n5.jpg\n45\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n50\n50.jpg\n46\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n51\n51.jpg\n47\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n52\n52.jpg\n48\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n53\n53.jpg\n49\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n54\n54.jpg\n50\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n55\n55.jpg\n51\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n56\n56.jpg\n52\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n57\n57.jpg\n53\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n58\n58.jpg\n54\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n59\n59.jpg\n55\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n6\n6.jpg\n56\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n60\n60.jpg\n57\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n61\n61.jpg\n58\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n62\n62.jpg\n59\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n63\n63.jpg\n60\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n64\n64.jpg\n61\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n65\n65.jpg\n62\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n66\n66.jpg\n63\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n67\n67.jpg\n64\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n68\n68.jpg\n65\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n69\n69.jpg\n66\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n7\n7.jpg\n67\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n70\n70.jpg\n68\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n71\n71.jpg\n69\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n72\n72.jpg\n70\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n73\n73.jpg\n71\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n74\n74.jpg\n72\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n75\n75.jpg\n73\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n76\n76.jpg\n74\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n77\n77.jpg\n75\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n78\n78.jpg\n76\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n79\n79.jpg\n77\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n8\n8.jpg\n78\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n80\n80.jpg\n79\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n81\n81.jpg\n80\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n82\n82.jpg\n81\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n83\n83.jpg\n82\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n84\n84.jpg\n83\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n85\n85.jpg\n84\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n86\n86.jpg\n85\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n87\n87.jpg\n86\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n88\n88.jpg\n87\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n89\n89.jpg\n88\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n9\n9.jpg\n89\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n90\n90.jpg\n90\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n91\n91.jpg\n91\nNilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n92\n92.jpg\n92\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n93\n93.jpg\n93\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n94\n94.jpg\n94\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n95\n95.jpg\n95\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n96\n96.jpg\n96\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n97\n97.jpg\n97\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n98\n98.jpg\n98\nNot_Nilgai\n\n{0: 'Nilgai', 1: 'Not_Nilgai'}\n99\n99.jpg\n99\nNot_Nilgai\n\n"
    }
   ],
   "source": [
    "## Map prediction name to labels, move images to repsective folders, and print results\n",
    "## Todo: make this a function\n",
    "for i, logits in enumerate(pred):\n",
    "\n",
    "    conf = pd.DataFrame({'Prediction':list(pred)})\n",
    "    conf = conf.Prediction.apply(pd.Series)\n",
    "    conf = conf.max(axis=1)\n",
    "\n",
    "\n",
    "    class_idx = tf.round(pred).numpy().flatten()\n",
    "    labels = {'Nilgai': 0, 'Not_Nilgai': 1}\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    predictions = [labels[k] for k in predicted_class_indices]\n",
    "    predictions = predictions[i]\n",
    "\n",
    "    img_name = os.path.split(test_generator.filenames[i])[1]\n",
    "    samp_name = os.path.splitext(img_name)[0]\n",
    "    img = os.path.join(img_dir,img_name)\n",
    "    dst = os.path.join(pred_dir, predictions)\n",
    "    shutil.copy(img, dst)\n",
    "    \n",
    "    # print(\"{} prediction: {} ({:4.2f}%)\".format(str(samp_name), str(name), 100*pred[i].item()))\n",
    "    print(labels)\n",
    "    print(samp_name)\n",
    "    print(img_name)\n",
    "    print(i)\n",
    "    print(predictions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hello\n"
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9999659"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "conf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittf21conda2d5e8e5529ee4229b3fd9d7f11fc5f96",
   "display_name": "Python 3.7.7 64-bit ('tf2.1': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}